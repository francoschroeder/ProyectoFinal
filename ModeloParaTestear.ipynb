{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperpar√°metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path          = \"jena_climate_2009_2016.csv\"\n",
    "predicted_feature = 'T (degC)'\n",
    "\n",
    "train_perc = .715\n",
    "val_perc   = .285\n",
    "\n",
    "sequence_length = 24\n",
    "offset          = 1\n",
    "sampling_rate   = 6\n",
    "batch_size      = 256\n",
    "\n",
    "learning_rate   = 0.001\n",
    "epochs          = 10\n",
    "loss            = \"mse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(csv_path, parse_dates=True, index_col=0)\n",
    "data = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = data.drop(['p (mbar)','wv (m/s)', 'max. wv (m/s)', 'wd (deg)'],axis = 1)\n",
    "df = data.drop([\"Date Time\",\n",
    "                \"Tpot (K)\",\n",
    "                \"Tdew (degC)\",\n",
    "                \"rh (%)\",\n",
    "                \"VPact (mbar)\",\n",
    "                \"H2OC (mmol/mol)\",\n",
    "                \"max. wv (m/s)\",\n",
    "                \"wd (deg)\",],axis = 1)\n",
    "\n",
    "df.index = data[\"Date Time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Add periodic time intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timestamp_s = df.index\n",
    "#timestamp_s = timestamp_s.map(pd.Timestamp.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 24*60*60\n",
    "year = 365.2425 * day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " #df['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    " #df['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    " #df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    " #df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_length:  420551\n",
      "train_length:  300693\n",
      "val_length:  420551\n"
     ]
    }
   ],
   "source": [
    "df_length = len(df)\n",
    "\n",
    "print(\"df_length: \", df_length)\n",
    "\n",
    "train_length = int(df_length*train_perc)\n",
    "val_length   = int(df_length*(train_perc+val_perc))\n",
    "\n",
    "print(\"train_length: \", train_length)\n",
    "print(\"val_length: \", val_length)\n",
    "\n",
    "train_mean = df[:train_length].mean()\n",
    "train_std  = df[:train_length].std()\n",
    "\n",
    "df = (df - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Normalize data\n",
    "- Only normalize data based on training data\n",
    "    - Notice you should only normalize the training data - because validation and test data could affect the normalization\n",
    "- Get the mean and standard deviation of the data\n",
    "    - HINT: Use **.mean()** and **.std()** on the dataframe.\n",
    "- Noramlize the data as follows\n",
    "    - **train_df = (train_df - train_mean) / train_std** (assuming naming fits)\n",
    "    - HINT: The transformation of validation and test data is done similarly with **train_mean** and **train_std**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[:train_length-1]\n",
    "#val_df = df[train_length:val_length-1]\n",
    "val_df = df[train_length:val_length]\n",
    "test_df = df[val_length:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Create datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "300843\n"
     ]
    }
   ],
   "source": [
    "start = sequence_length*sampling_rate + offset*sampling_rate\n",
    "end   = train_length + start\n",
    "\n",
    "print(start)\n",
    "print(end)\n",
    "\n",
    "x_train = train_df\n",
    "y_train = df[[predicted_feature]][start:end]\n",
    "\n",
    "dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_train.values,\n",
    "    y_train,\n",
    "    sequence_length = sequence_length,\n",
    "    sampling_rate = sampling_rate,\n",
    "    shuffle=True,\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300843\n",
      "119708\n",
      "119858\n"
     ]
    }
   ],
   "source": [
    "start = train_length + sequence_length*sampling_rate + offset*sampling_rate\n",
    "#end   = val_length + start\n",
    "x_end = len(val_df) - sequence_length*sampling_rate - offset*sampling_rate\n",
    "\n",
    "print(start)\n",
    "print(x_end)\n",
    "print(len(val_df))\n",
    "\n",
    "x_val = val_df[:x_end]\n",
    "y_val = df[[predicted_feature]][start:]\n",
    "\n",
    "dataset_val = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_val.values,\n",
    "    y_val,\n",
    "    sequence_length = sequence_length,\n",
    "    sampling_rate = sampling_rate,\n",
    "    shuffle=False,\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = val_length + sequence_length*sampling_rate + offset*sampling_rate\n",
    "#x_end = len(test_df) - sequence_length*sampling_rate - offset*sampling_rate\n",
    "\n",
    "#x_test = test_df[:x_end]\n",
    "#y_test = test_df[[predicted_feature]][start:]\n",
    "\n",
    "#dataset_test = keras.preprocessing.timeseries_dataset_from_array(\n",
    "#    x_test.values,\n",
    "#    y_test,\n",
    "#    sequence_length = sequence_length,\n",
    "#    sampling_rate = sampling_rate,\n",
    "#    shuffle=False,\n",
    "#    batch_size = batch_size\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, input_width=24, offset=0, predict_column='T (degC)'):\n",
    "    x = []\n",
    "    y = []\n",
    "    data_x = df.to_numpy()\n",
    "    data_y = df[predict_column].to_numpy()\n",
    "    \n",
    "    for i in range(input_width, len(data_x) - offset):\n",
    "        x.append(data_x[i - input_width:i,:])\n",
    "        y.append(data_y[i + offset])\n",
    "        \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return x, y.reshape(-1,1)\n",
    "\n",
    "#test_df_norm = (test_df - train_mean) / train_std\n",
    "\n",
    "\n",
    "#for batch in dataset_test.take(1):\n",
    "#    x, y = batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = test_df[5::6]\n",
    "#test_prueba = create_dataset(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Create model\n",
    "- Create the following model\n",
    "    - **model = models.Sequential()**\n",
    "    - **model.add(layers.LSTM(32, return_sequences=True, input_shape=train_ds[0].shape[1:]))**\n",
    "    - **model.add(layers.Dense(units=1))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataset_train.take(1):\n",
    "    x, y = batch\n",
    "    \n",
    "input_shape = x.shape[1], x.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 24, 7)]           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 24, 32)            5120      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 13,473\n",
      "Trainable params: 13,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.layers.Input(input_shape)\n",
    "lstm_layer = keras.layers.LSTM(32, return_sequences=True)(inputs)\n",
    "lstm_layer2 = keras.layers.LSTM(32)(lstm_layer)\n",
    "output = keras.layers.Dense(1)(lstm_layer2)\n",
    "\n",
    "model = keras.Model(inputs, output)\n",
    "model.compile(keras.optimizers.Adam(learning_rate), loss)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Train model\n",
    "- Compile and fit the model\n",
    "- Complie the model as follows\n",
    "    - **model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])**\n",
    "- Fit the model as follows\n",
    "    - **model.fit(x=train_ds[0], y=train_ds[1], validation_data=(val_ds[0], val_ds[1]), epochs=5)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 881/1175 [=====================>........] - ETA: 10s - loss: 0.0474"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset_train, epochs = epochs, validation_data=dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss(history, title):\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    epochs = range(len(loss))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "visualize_loss(history, \"Training and Validation Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13: Predict data\n",
    "- Apply the model on the test data\n",
    "    - HINT: Use **model.predict(x)**, where **x** is assigned to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = test_prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 14: Plot the result\n",
    "- Plot a window of the data predicted together with the actual data.\n",
    "- One way:\n",
    "    - **fig, ax = plt.subplots()**\n",
    "    - **ax.plot(y[i:i+96*2,0], c='g')**\n",
    "    - **ax.plot(pred[i:i+96*2,-1,0], c='r')**\n",
    "- It will plot a window of 96 hours, where you can index with **i** (**i=150** as an example) and **y** is the real values and **pred** are the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(y[0:100], c='g', label=\"Test Data\")\n",
    "ax.plot(y_pred[0:100], c='r', label=\"Prediction\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(y[0:100], c='g', label=\"Test Data\")\n",
    "ax.plot(y_pred[0:100], c='r', label=\"Prediction\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(y[200:300], c='g', label=\"Test Data\")\n",
    "ax.plot(y_pred[200:300], c='r', label=\"Prediction\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(y[1000:1100], c='g', label=\"Test Data\")\n",
    "ax.plot(y_pred[1000:1100], c='r', label=\"Prediction\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(plot_data, delta, title):\n",
    "    labels = [\"History\", \"True Future\", \"Model Prediction\"]\n",
    "    marker = [\".-\", \"rx\", \"go\"]\n",
    "    time_steps = list(range(-(plot_data[0].shape[0]), 0))\n",
    "    if delta:\n",
    "        future = delta\n",
    "    else:\n",
    "        future = 0\n",
    "\n",
    "    plt.title(title)\n",
    "    for i, val in enumerate(plot_data):\n",
    "        if i:\n",
    "            plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])\n",
    "        else:\n",
    "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "    plt.legend()\n",
    "    plt.xlim([time_steps[0], (future + 5) * 2])\n",
    "    plt.xlabel(\"Time-Step\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "for x, y in dataset_test.take(5):\n",
    "    show_plot(\n",
    "        [x[0][:, 1].numpy(), y[0].numpy(), model.predict(x)[0]],\n",
    "        12,\n",
    "        \"Single Step Prediction\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

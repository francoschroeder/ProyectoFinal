{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperpar√°metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path          = \"jena_climate_2009_2016.csv\"\n",
    "predicted_feature = 0 #T (degC)\n",
    "\n",
    "train_perc = .7\n",
    "val_perc   = .2\n",
    "\n",
    "sequence_length = 24\n",
    "offset          = 1\n",
    "sampling_rate   = 6\n",
    "batch_size      = 256\n",
    "\n",
    "learning_rate   = 0.001\n",
    "epochs          = 10\n",
    "loss            = \"mse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(csv_path, parse_dates=True, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T (degC)</th>\n",
       "      <th>Tdew (degC)</th>\n",
       "      <th>rh (%)</th>\n",
       "      <th>VPmax (mbar)</th>\n",
       "      <th>VPact (mbar)</th>\n",
       "      <th>VPdef (mbar)</th>\n",
       "      <th>sh (g/kg)</th>\n",
       "      <th>H2OC (mmol/mol)</th>\n",
       "      <th>rho (g/m**3)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-01-01 00:10:00</th>\n",
       "      <td>-8.02</td>\n",
       "      <td>-8.90</td>\n",
       "      <td>93.3</td>\n",
       "      <td>3.33</td>\n",
       "      <td>3.11</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.94</td>\n",
       "      <td>3.12</td>\n",
       "      <td>1307.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-01 00:20:00</th>\n",
       "      <td>-8.41</td>\n",
       "      <td>-9.28</td>\n",
       "      <td>93.4</td>\n",
       "      <td>3.23</td>\n",
       "      <td>3.02</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.89</td>\n",
       "      <td>3.03</td>\n",
       "      <td>1309.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-01 00:30:00</th>\n",
       "      <td>-8.51</td>\n",
       "      <td>-9.31</td>\n",
       "      <td>93.9</td>\n",
       "      <td>3.21</td>\n",
       "      <td>3.01</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.88</td>\n",
       "      <td>3.02</td>\n",
       "      <td>1310.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-01 00:40:00</th>\n",
       "      <td>-8.31</td>\n",
       "      <td>-9.07</td>\n",
       "      <td>94.2</td>\n",
       "      <td>3.26</td>\n",
       "      <td>3.07</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1309.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-01 00:50:00</th>\n",
       "      <td>-8.27</td>\n",
       "      <td>-9.04</td>\n",
       "      <td>94.1</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.08</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3.09</td>\n",
       "      <td>1309.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     T (degC)  Tdew (degC)  rh (%)  VPmax (mbar)  \\\n",
       "Date Time                                                          \n",
       "2009-01-01 00:10:00     -8.02        -8.90    93.3          3.33   \n",
       "2009-01-01 00:20:00     -8.41        -9.28    93.4          3.23   \n",
       "2009-01-01 00:30:00     -8.51        -9.31    93.9          3.21   \n",
       "2009-01-01 00:40:00     -8.31        -9.07    94.2          3.26   \n",
       "2009-01-01 00:50:00     -8.27        -9.04    94.1          3.27   \n",
       "\n",
       "                     VPact (mbar)  VPdef (mbar)  sh (g/kg)  H2OC (mmol/mol)  \\\n",
       "Date Time                                                                     \n",
       "2009-01-01 00:10:00          3.11          0.22       1.94             3.12   \n",
       "2009-01-01 00:20:00          3.02          0.21       1.89             3.03   \n",
       "2009-01-01 00:30:00          3.01          0.20       1.88             3.02   \n",
       "2009-01-01 00:40:00          3.07          0.19       1.92             3.08   \n",
       "2009-01-01 00:50:00          3.08          0.19       1.92             3.09   \n",
       "\n",
       "                     rho (g/m**3)  \n",
       "Date Time                          \n",
       "2009-01-01 00:10:00       1307.75  \n",
       "2009-01-01 00:20:00       1309.80  \n",
       "2009-01-01 00:30:00       1310.24  \n",
       "2009-01-01 00:40:00       1309.19  \n",
       "2009-01-01 00:50:00       1309.00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data.drop(['Tpot (K)', 'p (mbar)','wv (m/s)', 'max. wv (m/s)', 'wd (deg)'],axis = 1)\n",
    "#df = data.drop([\"Tpot (K)\",\n",
    "#                \"Tdew (degC)\",\n",
    "#                \"rh (%)\",\n",
    "#                \"VPact (mbar)\",\n",
    "#                \"H2OC (mmol/mol)\",\n",
    "#                \"max. wv (m/s)\",\n",
    "#                \"wd (deg)\",],axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Add periodic time intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_s = df.index\n",
    "timestamp_s = timestamp_s.map(pd.Timestamp.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 24*60*60\n",
    "year = 365.2425 * day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "df['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_length = len(df)\n",
    "\n",
    "train_length = int(df_length*train_perc)\n",
    "val_length   = int(df_length*(train_perc+val_perc))\n",
    "\n",
    "train_mean = df.values[:train_length].mean()\n",
    "train_std  = df.values[:train_length].std()\n",
    "\n",
    "df_norm = (df.values - train_mean) / train_std\n",
    "df_norm = pd.DataFrame(df_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Normalize data\n",
    "- Only normalize data based on training data\n",
    "    - Notice you should only normalize the training data - because validation and test data could affect the normalization\n",
    "- Get the mean and standard deviation of the data\n",
    "    - HINT: Use **.mean()** and **.std()** on the dataframe.\n",
    "- Noramlize the data as follows\n",
    "    - **train_df = (train_df - train_mean) / train_std** (assuming naming fits)\n",
    "    - HINT: The transformation of validation and test data is done similarly with **train_mean** and **train_std**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_norm[:train_length-1]\n",
    "val_df   = df_norm[train_length:val_length-1]\n",
    "test_df_norm  = df_norm[val_length:]\n",
    "test_df       = df[val_length:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Create datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = sequence_length*sampling_rate + offset*sampling_rate\n",
    "end   = train_length + start\n",
    "\n",
    "x_train = train_df\n",
    "y_train = df_norm[[predicted_feature]][start:end]\n",
    "\n",
    "dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_train.values,\n",
    "    y_train,\n",
    "    sequence_length = sequence_length,\n",
    "    sampling_rate = sampling_rate,\n",
    "    shuffle=True,\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = train_length + sequence_length*sampling_rate + offset*sampling_rate\n",
    "end   = val_length + start\n",
    "\n",
    "x_val = val_df\n",
    "y_val = df_norm[[predicted_feature]][start:end]\n",
    "\n",
    "dataset_val = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_val.values,\n",
    "    y_val,\n",
    "    sequence_length = sequence_length,\n",
    "    sampling_rate = sampling_rate,\n",
    "    shuffle=False,\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = val_length + sequence_length*sampling_rate + offset*sampling_rate\n",
    "x_end = len(test_df_norm) - sequence_length*sampling_rate - offset*sampling_rate\n",
    "\n",
    "x_test = test_df_norm[:x_end]\n",
    "y_test = df_norm[[predicted_feature]][start:]\n",
    "\n",
    "dataset_test = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_test.values,\n",
    "    y_test,\n",
    "    sequence_length = sequence_length,\n",
    "    sequence_stride = sampling_rate,\n",
    "    sampling_rate = sampling_rate,\n",
    "    shuffle=False,\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Create model\n",
    "- Create the following model\n",
    "    - **model = models.Sequential()**\n",
    "    - **model.add(layers.LSTM(32, return_sequences=True, input_shape=train_ds[0].shape[1:]))**\n",
    "    - **model.add(layers.Dense(units=1))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataset_train.take(1):\n",
    "    x, y = batch\n",
    "    \n",
    "input_shape = x.shape[1], x.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 24, 13)]          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                5888      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 5,921\n",
      "Trainable params: 5,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.layers.Input(input_shape)\n",
    "#lstm_layer = keras.layers.LSTM(32, return_sequences=True)(inputs)\n",
    "lstm_layer2 = keras.layers.LSTM(32)(inputs)\n",
    "output = keras.layers.Dense(1)(lstm_layer2)\n",
    "\n",
    "model = keras.Model(inputs, output)\n",
    "model.compile(keras.optimizers.Adam(learning_rate), loss)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Train model\n",
    "- Compile and fit the model\n",
    "- Complie the model as follows\n",
    "    - **model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])**\n",
    "- Fit the model as follows\n",
    "    - **model.fit(x=train_ds[0], y=train_ds[1], validation_data=(val_ds[0], val_ds[1]), epochs=5)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1150/1150 [==============================] - 20s 15ms/step - loss: 4.2474e-04 - val_loss: 7.0071e-05\n",
      "Epoch 2/10\n",
      "1150/1150 [==============================] - 18s 16ms/step - loss: 4.8359e-05 - val_loss: 4.9900e-05\n",
      "Epoch 3/10\n",
      "1150/1150 [==============================] - 19s 16ms/step - loss: 3.5617e-05 - val_loss: 2.8490e-05\n",
      "Epoch 4/10\n",
      "1150/1150 [==============================] - 18s 16ms/step - loss: 3.2293e-05 - val_loss: 2.4183e-05\n",
      "Epoch 5/10\n",
      "1150/1150 [==============================] - 18s 15ms/step - loss: 2.9882e-05 - val_loss: 3.2254e-05\n",
      "Epoch 6/10\n",
      "1150/1150 [==============================] - 18s 15ms/step - loss: 2.7298e-05 - val_loss: 1.9525e-05\n",
      "Epoch 7/10\n",
      " 684/1150 [================>.............] - ETA: 6s - loss: 2.5954e-05"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset_train, epochs = epochs, validation_data=dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss(history, title):\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    epochs = range(len(loss))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "visualize_loss(history, \"Training and Validation Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13: Predict data\n",
    "- Apply the model on the test data\n",
    "    - HINT: Use **model.predict(x)**, where **x** is assigned to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataset_test.take(5):\n",
    "    x, y = batch\n",
    "    \n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    y_pred = y_pred * train_std + train_mean\n",
    "    y = y * train_std + train_mean\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.plot(y[0:100], c='g', label=\"Test Data\")\n",
    "    ax.plot(y_pred[0:100], c='r', label=\"Prediction\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 14: Plot the result\n",
    "- Plot a window of the data predicted together with the actual data.\n",
    "- One way:\n",
    "    - **fig, ax = plt.subplots()**\n",
    "    - **ax.plot(y[i:i+96*2,0], c='g')**\n",
    "    - **ax.plot(pred[i:i+96*2,-1,0], c='r')**\n",
    "- It will plot a window of 96 hours, where you can index with **i** (**i=150** as an example) and **y** is the real values and **pred** are the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(y[0:256], c='g', label=\"Test Data\")\n",
    "ax.plot(y_pred[0:256], c='r', label=\"Prediction\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(y_real[500:600], c='g', label=\"Test Data\")\n",
    "ax.plot(y_pred[500:600], c='r', label=\"Prediction\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(y_real[200:300], c='g', label=\"Test Data\")\n",
    "ax.plot(y_pred[200:300], c='r', label=\"Prediction\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(y_real[950:1050], c='g', label=\"Test Data\")\n",
    "ax.plot(y_pred[950:1050], c='r', label=\"Prediction\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(y_real[1000:1100], c='g', label=\"Test Data\")\n",
    "ax.plot(y_pred[1000:1100], c='r', label=\"Prediction\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(plot_data, delta, title):\n",
    "    labels = [\"History\", \"True Future\", \"Model Prediction\"]\n",
    "    marker = [\".-\", \"rx\", \"go\"]\n",
    "    time_steps = list(range(-(plot_data[0].shape[0]), 0))\n",
    "    if delta:\n",
    "        future = delta\n",
    "    else:\n",
    "        future = 0\n",
    "\n",
    "    plt.title(title)\n",
    "    for i, val in enumerate(plot_data):\n",
    "        if i:\n",
    "            plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])\n",
    "        else:\n",
    "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "    plt.legend()\n",
    "    plt.xlim([time_steps[0], (future + 5) * 2])\n",
    "    plt.xlabel(\"Time-Step\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "for x1, y1 in dataset_test.take(5):\n",
    "    show_plot(\n",
    "        [x1[0][:, 1].numpy(), y1[0].numpy(), model.predict(x1)[0]],\n",
    "        12,\n",
    "        \"Single Step Prediction\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

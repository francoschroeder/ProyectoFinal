{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Librerías importadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path          = \"regina.csv\"\n",
    "dropped_features  = ['Hi Temp', 'Low Temp', 'Wind Chill', 'Heat Index', 'THSW Index', 'THW Index', 'Wind Run', 'Solar Energy', 'Hi Solar Rad.', 'In Heat', 'ISS Recept', 'Arc. Int']\n",
    "\n",
    "train_perc = .8\n",
    "val_perc   = .1\n",
    "\n",
    "sequence_length = 90\n",
    "offset          = 0\n",
    "sampling_rate   = 1\n",
    "length          = 6 #horas\n",
    "min_temp        = 0.5\n",
    "batch_size      = 256\n",
    "\n",
    "learning_rate   = 0.001\n",
    "epochs          = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateparse = lambda x: datetime.strptime(x, '%d.%m.%y %H:%M')\n",
    "\n",
    "wind_dic = {'---' : 0,\n",
    "            'E': 1,\n",
    "            'W': 2,\n",
    "            'N': 3,\n",
    "            'S': 4,\n",
    "            'NE': 5,\n",
    "            'SE': 6,\n",
    "            'NW': 7,\n",
    "            'SW': 8,\n",
    "            'ENE': 9,\n",
    "            'NNE': 10,\n",
    "            'WNW': 11,\n",
    "            'NNW': 12,\n",
    "            'ESE': 13,\n",
    "            'SSE': 14,\n",
    "            'WSW': 15,\n",
    "            'SSW': 16}\n",
    "\n",
    "data = pd.read_csv(csv_path, parse_dates=['Date Time'], date_parser=dateparse, na_values=['---', '------'], converters={'Wind Dir': lambda x: wind_dic[x], 'Hi Dir': lambda x: wind_dic[x]}, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregamos los datos que faltan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data['Temp Out'] = data['Temp Out'].apply(lambda x: x-2)\n",
    "data['Temp Out'] = data['Temp Out'].apply(lambda x: np.nan if x <= -10 else x)\n",
    "\n",
    "data = data.resample('10min', origin='start').mean()\n",
    "\n",
    "data['Wind Dir'] = data['Wind Dir'].replace(np.nan, 0)\n",
    "data['Hi Dir'] = data['Hi Dir'].replace(np.nan, 0)\n",
    "\n",
    "data = data.interpolate()\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertimos estampillas de tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_s = data.index\n",
    "timestamp_s = timestamp_s.map(pd.Timestamp.timestamp)\n",
    "\n",
    "day = 24*60*60\n",
    "year = 365.2425 * day\n",
    "\n",
    "data['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "data['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "data['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "data['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropeamos las características que no tienen correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.drop(dropped_features, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_length = len(df)\n",
    "\n",
    "train_length = int(df_length*train_perc)\n",
    "val_length   = int(df_length*(train_perc+val_perc))\n",
    "\n",
    "train_mean = df.values[:train_length].mean(axis=0)\n",
    "train_std  = df.values[:train_length].std(axis=0)\n",
    "\n",
    "df_norm = (df.values - train_mean) / train_std\n",
    "df_norm = pd.DataFrame(df_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"norm.params\", \"wb\")\n",
    "pickle.dump([train_mean, train_std, wind_dic], file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividisión de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_norm[:train_length-1]\n",
    "val_df   = df_norm[train_length:val_length-1]\n",
    "test_df  = df_norm[val_length:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Creación de secuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_y(df, start, end, length, min_temp):\n",
    "    y_data = np.arange(end-start)\n",
    "    \n",
    "    df_bool = df['Temp Out'][start:end+length] <= min_temp\n",
    "        \n",
    "    for i in range(0, end-start):\n",
    "        y_data[i] = 1 if np.any(df_bool[i:i+length]) else 0\n",
    "\n",
    "    return y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = sequence_length*sampling_rate + offset*sampling_rate\n",
    "end   = train_length + offset*sampling_rate\n",
    "\n",
    "x_train = train_df\n",
    "y_train = create_y(df, start, end, length*6, min_temp)\n",
    "\n",
    "dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_train.values,\n",
    "    y_train,\n",
    "    sequence_length = sequence_length,\n",
    "    sampling_rate = sampling_rate,\n",
    "    shuffle=True,\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = train_length + sequence_length*sampling_rate + offset*sampling_rate\n",
    "end   = val_length + offset*sampling_rate\n",
    "\n",
    "x_val = val_df\n",
    "y_val = create_y(df, start, end, length*6, min_temp)\n",
    "\n",
    "dataset_val = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_val.values,\n",
    "    y_val,\n",
    "    sequence_length = sequence_length,\n",
    "    sampling_rate = sampling_rate,\n",
    "    shuffle=False,\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = val_length + sequence_length*sampling_rate + offset*sampling_rate\n",
    "x_end = len(test_df) - sequence_length*sampling_rate - offset*sampling_rate\n",
    "\n",
    "x_test = test_df[:x_end]\n",
    "y_test = create_y(df, start, df_length, length*6, min_temp)\n",
    "\n",
    "dataset_test = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_test.values,\n",
    "    y_test,\n",
    "    sequence_length = sequence_length,\n",
    "    sequence_stride = sampling_rate,\n",
    "    sampling_rate = sampling_rate,\n",
    "    shuffle=False,\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataset_train.take(1):\n",
    "    x, y = batch\n",
    "\n",
    "input_shape = x.shape[1], x.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(input_shape)\n",
    "lstm_layer = keras.layers.LSTM(4, dropout=0.2, recurrent_dropout=0.2, return_sequences=False)(inputs)\n",
    "output = keras.layers.Dense(1, activation=\"sigmoid\")(lstm_layer)\n",
    "\n",
    "model = keras.Model(inputs, output)\n",
    "model.compile(keras.optimizers.Adam(learning_rate), loss=tf.keras.losses.BinaryCrossentropy(from_logits=False))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path_checkpoint = \"modelo_pronosticador_regina.h5\"\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n",
    "\n",
    "modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    filepath=path_checkpoint,\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "history = model.fit(dataset_train, epochs = epochs, validation_data=dataset_val, callbacks = [es_callback, modelckpt_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [6, 4]\n",
    "\n",
    "def visualize_loss(history, title):\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    epochs = range(len(loss))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.ylim(0.05, 0.18)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "visualize_loss(history, \"Training and Validation Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostramos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = start\n",
    "\n",
    "test_real_y = np.zeros(0)\n",
    "test_pred_y = np.zeros(0)\n",
    "fechas  = np.zeros(0, dtype='datetime64')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "\n",
    "test_size  = tf.data.experimental.cardinality(dataset_test).numpy()\n",
    "\n",
    "for batch in dataset_test.take(test_size):\n",
    "    x, y = batch\n",
    "    y_pred = model.predict(x)\n",
    "    \n",
    "    b_size = x.shape[0]\n",
    "    \n",
    "    test_real_y  = np.concatenate((test_real_y, y))\n",
    "    test_pred_y  = np.concatenate((test_pred_y, np.squeeze(y_pred)))\n",
    "    fechas  = np.concatenate((fechas, np.squeeze(np.array(data.index)[i:i+b_size])))\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.plot(data.index[i:i+b_size], y[0:b_size], c='r', label=\"Test Data\")\n",
    "    ax.plot(data.index[i:i+b_size], (y_pred[0:b_size]),c='g', label=\"Prediction\")\n",
    "    \n",
    "    i += b_size\n",
    "    \n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = tf.data.experimental.cardinality(dataset_train).numpy()\n",
    "val_size   = tf.data.experimental.cardinality(dataset_val).numpy()\n",
    "test_size = (test_size-1)*batch_size + b_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_real_y = np.zeros(0)\n",
    "train_pred_y = np.zeros(0)\n",
    "\n",
    "for batch in dataset_train.take(train_size):\n",
    "    x, y = batch\n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    last_batch_size = x.shape[0]\n",
    "    \n",
    "    train_real_y  = np.concatenate((train_real_y, y))\n",
    "    train_pred_y  = np.concatenate((train_pred_y, np.squeeze(y_pred)))\n",
    "    \n",
    "train_size = (train_size-1)*batch_size + last_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_real_y = np.zeros(0)\n",
    "val_pred_y = np.zeros(0)\n",
    "\n",
    "for batch in dataset_val.take(val_size):\n",
    "    x, y = batch\n",
    "    y_pred = model.predict(x)\n",
    "    \n",
    "    last_batch_size = x.shape[0]\n",
    "\n",
    "    val_real_y  = np.concatenate((val_real_y, y))\n",
    "    val_pred_y  = np.concatenate((val_pred_y, np.squeeze(y_pred)))\n",
    "\n",
    "val_size = (val_size-1)*batch_size + last_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_train = confusion_matrix(train_real_y, np.around(train_pred_y))\n",
    "conf_matrix_val   = confusion_matrix(val_real_y, np.around(val_pred_y))\n",
    "conf_matrix_test  = confusion_matrix(test_real_y, np.around(test_pred_y)) \n",
    "\n",
    "ConfusionMatrixDisplay(conf_matrix_test, display_labels=['No Heló', 'Heló']).plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_train, fp_train, fn_train, tp_train = conf_matrix_train.ravel()\n",
    "tn_val, fp_val, fn_val, tp_val         = conf_matrix_val.ravel()\n",
    "tn_test, fp_test, fn_test, tp_test     = conf_matrix_test.ravel()\n",
    "\n",
    "def accuracy(tn, fp, fn, tp): return (tp+tn)/(tp+tn+fp+fn)\n",
    "def precision(tp, fp): return tp/(tp+fp)\n",
    "def recall(tp, fn): return tp/(tp+fn)\n",
    "def f1(tp, fp, fn): return (2*tp)/(2*tp+fp+fn)\n",
    "\n",
    "stats = pd.DataFrame({'Dataset':   ['train', 'val', 'test'],\n",
    "                      'Size':      [train_size, val_size, test_size],\n",
    "                      'Accuracy':  [accuracy(tn_train, fp_train, fn_train, tp_train), accuracy(tn_val, fp_val, fn_val, tp_val), accuracy(tn_test, fp_test, fn_test, tp_test)],\n",
    "                      'Precision': [precision(tp_train, fp_train), precision(tp_val, fp_val), precision(tp_test, fp_test)],\n",
    "                      'Recall':    [recall(tp_train, fn_train), recall(tp_val, fn_val), recall(tp_test, fn_test)],\n",
    "                      'F1':        [f1(tp_train, fp_train, fn_train), f1(tp_val, fp_val, fn_val), f1(tp_test, fp_test, fn_test)]})\n",
    "\n",
    "stats.style.hide_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fechas = pd.to_datetime(fechas)\n",
    "\n",
    "indexes = np.squeeze(np.argwhere((fechas.hour >= 20) & (fechas.hour <= 23)))\n",
    "\n",
    "noches_real  = test_real_y[indexes]\n",
    "noches_pred  = test_pred_y[indexes]\n",
    "fechas_noche = fechas[indexes]\n",
    "\n",
    "indexes = np.squeeze(np.argwhere((fechas_noche.hour == 23) & (fechas_noche.minute == 50)))\n",
    "\n",
    "noches_real_split = np.split(noches_real, indexes)\n",
    "noches_pred_split = np.split(noches_pred, indexes)\n",
    "\n",
    "print('METRICAS')\n",
    "print('Estadísticas teniendo en cuenta el período de 20hs a 00hs')\n",
    "print()\n",
    "print('Dataset Test:')\n",
    "print('Cantidad de noches predecidas / Cantidad de noches que heló: ', sum((np.any(noches_real_split[i] == 1) & (np.any(pred >= 0.5))) for i, pred in enumerate(noches_pred_split)), \" / \", sum(np.any(x == 1) for x in noches_real_split))\n",
    "print('Cantidad de noches con falsos positivos / Cantidad de noches que no heló: ', sum((np.all(noches_real_split[i] == 0) & (np.any(pred >= 0.5))) for i, pred in enumerate(noches_pred_split)), \" / \", sum(np.all(x == 0) for x in noches_real_split))\n",
    "print()\n",
    "\n",
    "noches_criticas_real = noches_real[(fechas_noche.month == 8) | (fechas_noche.month == 9) | (fechas_noche.month == 10)]\n",
    "noches_criticas_pred = noches_pred[(fechas_noche.month == 8) | (fechas_noche.month == 9) | (fechas_noche.month == 10)]\n",
    "fechas_criticas      = fechas_noche[(fechas_noche.month == 8) | (fechas_noche.month == 9) | (fechas_noche.month == 10)]\n",
    "\n",
    "indexes = np.squeeze(np.argwhere((fechas_criticas.hour == 23) & (fechas_criticas.minute == 50)))\n",
    "\n",
    "noches_criticas_real = np.split(noches_criticas_real, indexes)\n",
    "noches_criticas_pred = np.split(noches_criticas_pred, indexes)\n",
    "\n",
    "print('Dataset Test (meses 8, 9 y 10):')\n",
    "print('Cantidad de noches predecidas / Cantidad de noches que heló: ', sum((np.any(noches_criticas_real[i] == 1) & (np.any(pred >= 0.5))) for i, pred in enumerate(noches_criticas_pred)), \" / \", sum(np.any(x == 1) for x in noches_criticas_real))\n",
    "print('Cantidad de noches con falsos positivos / Cantidad de noches que no heló: ', sum((np.all(noches_criticas_real[i] == 0) & (np.any(pred >= 0.5))) for i, pred in enumerate(noches_criticas_pred)), \" / \", sum(np.all(x == 0) for x in noches_criticas_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

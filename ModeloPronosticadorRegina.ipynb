{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path          = \"regina.csv\"\n",
    "dropped_features  = ['Hi Temp', 'Low Temp', 'Wind Chill', 'Heat Index', 'THSW Index', 'THW Index', 'Wind Run', 'Solar Energy', 'Hi Solar Rad.', 'In Heat', 'ISS Recept', 'Arc. Int']\n",
    "\n",
    "train_perc = .8\n",
    "val_perc   = .1\n",
    "\n",
    "sequence_length = 90\n",
    "offset          = 0\n",
    "sampling_rate   = 1\n",
    "length          = 6 #horas\n",
    "min_temp        = 0.5\n",
    "batch_size      = 256\n",
    "\n",
    "learning_rate   = 0.001\n",
    "epochs          = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateparse = lambda x: datetime.strptime(x, '%d.%m.%y %H:%M')\n",
    "\n",
    "wind_dic = {'---' : 0,\n",
    "            'E': 1,\n",
    "            'W': 2,\n",
    "            'N': 3,\n",
    "            'S': 4,\n",
    "            'NE': 5,\n",
    "            'SE': 6,\n",
    "            'NW': 7,\n",
    "            'SW': 8,\n",
    "            'ENE': 9,\n",
    "            'NNE': 10,\n",
    "            'WNW': 11,\n",
    "            'NNW': 12,\n",
    "            'ESE': 13,\n",
    "            'SSE': 14,\n",
    "            'WSW': 15,\n",
    "            'SSW': 16}\n",
    "\n",
    "data = pd.read_csv(csv_path, parse_dates=['Date Time'], date_parser=dateparse, na_values=['---', '------'], converters={'Wind Dir': lambda x: wind_dic[x], 'Hi Dir': lambda x: wind_dic[x]}, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregamos los datos que faltan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data['Temp Out'] = data['Temp Out'].apply(lambda x: x-2)\n",
    "data['Temp Out'] = data['Temp Out'].apply(lambda x: np.nan if x <= -10 else x)\n",
    "\n",
    "data = data.resample('10min', origin='start').mean()\n",
    "\n",
    "data['Wind Dir'] = data['Wind Dir'].replace(np.nan, 0)\n",
    "data['Hi Dir'] = data['Hi Dir'].replace(np.nan, 0)\n",
    "\n",
    "data = data.interpolate()\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertimos estampillas de tiempo todo a segundos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_s = data.index\n",
    "timestamp_s = timestamp_s.map(pd.Timestamp.timestamp)\n",
    "\n",
    "day = 24*60*60\n",
    "year = 365.2425 * day\n",
    "\n",
    "data['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "data['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "data['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "data['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropeamos las características que no tienen correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.drop(dropped_features, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_length = len(df)\n",
    "\n",
    "train_length = int(df_length*train_perc)\n",
    "val_length   = int(df_length*(train_perc+val_perc))\n",
    "\n",
    "train_mean = df.values[:train_length].mean(axis=0)\n",
    "train_std  = df.values[:train_length].std(axis=0)\n",
    "\n",
    "df_norm = (df.values - train_mean) / train_std\n",
    "df_norm = pd.DataFrame(df_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividimos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_norm[:train_length-1]\n",
    "val_df   = df_norm[train_length:val_length-1]\n",
    "test_df  = df_norm[val_length:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_y(df, start, end, length, min_temp):\n",
    "    y_data = np.arange(end-start)\n",
    "    \n",
    "    df_bool = df['Temp Out'][start:end+length] <= min_temp\n",
    "        \n",
    "    for i in range(0, end-start):\n",
    "        y_data[i] = 1 if np.any(df_bool[i:i+length]) else 0\n",
    "\n",
    "    return y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = sequence_length*sampling_rate + offset*sampling_rate\n",
    "end   = train_length + offset*sampling_rate\n",
    "\n",
    "x_train = train_df\n",
    "y_train = create_y(df, start, end, length*6, min_temp)\n",
    "\n",
    "dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_train.values,\n",
    "    y_train,\n",
    "    sequence_length = sequence_length,\n",
    "    sampling_rate = sampling_rate,\n",
    "    shuffle=True,\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = train_length + sequence_length*sampling_rate + offset*sampling_rate\n",
    "end   = val_length + offset*sampling_rate\n",
    "\n",
    "x_val = val_df\n",
    "y_val = create_y(df, start, end, length*6, min_temp)\n",
    "\n",
    "dataset_val = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_val.values,\n",
    "    y_val,\n",
    "    sequence_length = sequence_length,\n",
    "    sampling_rate = sampling_rate,\n",
    "    shuffle=False,\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = val_length + sequence_length*sampling_rate + offset*sampling_rate\n",
    "x_end = len(test_df) - sequence_length*sampling_rate - offset*sampling_rate\n",
    "\n",
    "x_test = test_df[:x_end]\n",
    "y_test = create_y(df, start, df_length, length*6, min_temp)\n",
    "\n",
    "dataset_test = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_test.values,\n",
    "    y_test,\n",
    "    sequence_length = sequence_length,\n",
    "    sequence_stride = sampling_rate,\n",
    "    sampling_rate = sampling_rate,\n",
    "    shuffle=False,\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataset_train.take(1):\n",
    "    x, y = batch\n",
    "    \n",
    "input_shape = x.shape[1], x.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(input_shape)\n",
    "lstm_layer = keras.layers.LSTM(8, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)(inputs)\n",
    "lstm_layer2 = keras.layers.LSTM(8, dropout=0.3, recurrent_dropout=0.3)(lstm_layer)\n",
    "output = keras.layers.Dense(1, activation=\"sigmoid\")(lstm_layer2)\n",
    "\n",
    "model = keras.Model(inputs, output)\n",
    "model.compile(keras.optimizers.Adam(learning_rate), loss=tf.keras.losses.BinaryCrossentropy(from_logits=False))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path_checkpoint = \"model_test_regina.h5\"\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n",
    "\n",
    "modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    filepath=path_checkpoint,\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "history = model.fit(dataset_train, epochs = epochs, validation_data=dataset_val, callbacks = [es_callback, modelckpt_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss(history, title):\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    epochs = range(len(loss))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "visualize_loss(history, \"Training and Validation Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostramos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = start\n",
    "\n",
    "print(data.index[start])\n",
    "\n",
    "real_y = np.zeros(0)\n",
    "pred_y = np.zeros(0)\n",
    "\n",
    "metrica = np.zeros(0)\n",
    "fechas  = np.zeros(0, dtype='datetime64')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "\n",
    "for batch in dataset_test.take(244):\n",
    "    x, y = batch\n",
    "    y_pred = model.predict(x)\n",
    "    \n",
    "    real_y  = np.concatenate((real_y, y))\n",
    "    pred_y  = np.concatenate((pred_y, np.squeeze(y_pred)))\n",
    "    \n",
    "    indexes = np.argwhere(y)\n",
    "    if not indexes.size == 0:\n",
    "        metrica = np.concatenate((metrica, np.squeeze(y_pred[indexes])))\n",
    "        fechas  = np.concatenate((fechas, np.squeeze(np.array(data.index)[indexes+i])))\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.plot(data.index[i:i+256], y[0:256], c='r', label=\"Test Data\")\n",
    "    ax.plot(data.index[i:i+256], (y_pred[0:256]),c='g', label=\"Prediction\")\n",
    "    \n",
    "    i += 256\n",
    "    \n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(real_y, np.around(pred_y), display_labels=['No Heló', 'Heló'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrica.shape)\n",
    "print(fechas.shape)\n",
    "\n",
    "print(metrica)\n",
    "print(fechas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
